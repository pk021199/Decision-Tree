{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        " - A Decision Tree is a supervised machine learning model that is widely used  for both classification and regression tasks. In the context of classification, it works by splitting data into subsets based on feature values, forming a tree-like structure that leads to decisions about class labels.\n",
        "\n",
        "     How it works in classification:\n",
        "     1. Root Node (Starting Point):\n",
        "\n",
        "        The tree starts at the root, which represents the entire dataset. At this point, the algorithm looks for the \"best feature\" to split the data on.\n",
        "     2. Splitting Criteria:\n",
        "         * The algorithm chooses a feature and a threshold that best separates the data into classes.\n",
        "         * Common measures for this selection include:\n",
        "           * Gini Impurity\n",
        "           * Entropy (Information Gain)\n",
        "           * Classification Error\n",
        "     3. Internal Nodes (Decision Points):\n",
        "        \n",
        "        Each node in the tree represents a feature test (e.g., \"Is Age > 30?\"). Based on the answer (Yes/No), the data flows down different branches.\n",
        "     4. Leaf Nodes (Outcomes):\n",
        "        \n",
        "        Eventually, the data reaches a leaf node, which assigns a class label (or probability distribution over classes). For example, \"Spam\" vs. \"Not Spam.\"\n",
        "     5. Prediction Process:\n",
        "        \n",
        "        To classify a new sample, the model starts at the root node and moves through the tree according to the sample's feature values until it reaches a leaf node. The class of that leaf node is the prediction.\n",
        "\n",
        "        Example:\n",
        "        \n",
        "        Suppose we want to classify whether someone will buy a product:\n",
        "        * Root Node: \"Is Age > 30?\"\n",
        "        * If Yes → Check \"Income level?\"\n",
        "        * If No → Check \"Student?\"\n",
        "        * Leaf Nodes: \"Buys Product\" or \"Does Not Buy Product.\"\n",
        "\n",
        "        Advantages:\n",
        "        * Easy to interpret and visualize.\n",
        "        * Handles both numerical and categorical data.\n",
        "        * Requires little preprocessing (no need for feature scaling).\n",
        "\n",
        "        Limitations:\n",
        "        * Can easily overfit if not pruned.\n",
        "        * Small changes in data can produce very different trees (high variance)."
      ],
      "metadata": {
        "id": "38LawRqd2OTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        " - Decision Trees split data based on impurity measures. Two of the most common are Gini Impurity and Entropy (Information Gain). Let’s break them down:\n",
        "\n",
        " 1. Gini Impurity\n",
        "     \n",
        "     Definition: Gini measures how often a randomly chosen sample would be incorrectly classified if it was randomly labeled according to the class distribution in the node.\n",
        "\n",
        "     Formula:\n",
        "     \n",
        "     For a node with classes C={1,2,...,k}:\n",
        "\n",
        "     Gini=1−i=1∑k​pi2\n",
        "\n",
        "     where pi is the proportion of samples belonging to class i at that node.\n",
        "\n",
        "  * Intuition:\n",
        "     * If all samples in a node belong to the same class → Gini = 0 (pure).\n",
        "     * Higher Gini means more mixed classes (impure).\n",
        " 2. Entropy (Information Gain)\n",
        "  * Definition: Entropy measures the \"disorder\" or \"uncertainty\" in a dataset. It comes from information theory.\n",
        "  * Formula:\n",
        "\n",
        "     Entropy=−i=1∑k​pi​log2​(pi​)\n",
        "\n",
        "     where pi is the proportion of samples belonging to class i.\n",
        "\n",
        "  * Intuition:\n",
        "     * Entropy = 0 → completely pure (all samples same class).\n",
        "     * Entropy = 1 (for binary classification with 50/50 split) → maximum uncertainty.\n",
        "  * Information Gain:\n",
        "     \n",
        "     Decision Trees using Entropy typically maximize Information Gain, which is the reduction in entropy after a split:\n",
        "\n",
        "     IG=Entropy(parent)−j∑​nnj​​⋅Entropy(childj​)\n",
        "\n",
        " 3. How They Impact Splits in a Decision Tree\n",
        "  * At each node, the algorithm evaluates possible splits on features and  chooses the split that results in the greatest reduction in impurity.\n",
        "     * Using Gini: Chooses the split with the lowest Gini after splitting.\n",
        "     * Using Entropy: Chooses the split with the highest Information Gain.\n",
        "  * Both generally give similar results:\n",
        "     * Gini is slightly faster (no logarithms).\n",
        "     * Entropy can be more sensitive to class imbalance.\n",
        "\n"
      ],
      "metadata": {
        "id": "PvkW31-l74fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        " - **Pre-Pruning (Early Stopping)**\n",
        "    * Definition: Stop growing the tree early before it becomes too complex.\n",
        "    * How it works: The algorithm imposes constraints during tree construction, such as:\n",
        "      * Maximum depth of the tree (max_depth)\n",
        "      * Minimum samples required to split a node (min_samples_split)\n",
        "      * Minimum samples in a leaf (min_samples_leaf)\n",
        "      * Minimum impurity decrease required for a split\n",
        "    * Advantage:\n",
        "      \n",
        "      Faster training & simpler models — avoids wasting time on splits that don’t add much value.\n",
        "    * Example:\n",
        "      \n",
        "      In customer churn prediction, you might restrict tree depth to prevent it from memorizing rare cases.\n",
        "\n",
        "  * **Post-Pruning (Cost Complexity Pruning / Reduced Error Pruning)**\n",
        "    * Definition: Stop growing the tree early before it becomes too complex.\n",
        "    * How it works: The algorithm imposes constraints during tree construction, such as:\n",
        "      * Maximum depth of the tree (max_depth)\n",
        "      * Minimum samples required to split a node (min_samples_split)\n",
        "      * Minimum samples in a leaf (min_samples_leaf)\n",
        "      * Minimum impurity decrease required for a split\n",
        "    * Advantage:\n",
        "     \n",
        "       Faster training & simpler models — avoids wasting time on splits that don’t add much value.\n",
        "    * Example:\n",
        "      \n",
        "      In customer churn prediction, you might restrict tree depth to prevent it from memorizing rare cases.\n",
        "\n",
        "  * **Post-Pruning (Cost Complexity Pruning / Reduced Error Pruning)**\n",
        "    * Definition: First grow a fully grown tree (possibly overfitted), then prune back by removing branches that don’t improve generalization.\n",
        "    * How it works:\n",
        "      * Start with a large tree.\n",
        "      * Evaluate subtrees on a validation set or using cost-complexity pruning (penalizing tree size).\n",
        "      * Remove branches that don’t improve accuracy.\n",
        "    * Advantage:\n",
        "      \n",
        "      Better generalization — because the tree has already explored complex patterns, pruning ensures only useful branches remain.\n",
        "    * Example:\n",
        "      \n",
        "      In medical diagnosis, post-pruning ensures the tree doesn’t overfit to rare, noisy patient cases while still capturing important patterns.\n",
        "\n",
        "  * **Main Difference:**\n",
        "    * Pre-pruning: Prevents the tree from growing too large in the first place (early stopping).\n",
        "    * Post-pruning: Grows the tree fully, then trims it back (after training).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mI6kBFrO74cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. : What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        " -  Definition:\n",
        "     \n",
        "     Information Gain measures the reduction in uncertainty (entropy) about the target variable after splitting a dataset on a given feature.\n",
        "   * Formula:\n",
        "\n",
        "     IG(S,A)=Entropy(S)−v∈Values(A)∑​∣S∣∣Sv​∣​⋅Entropy(Sv​)\n",
        "\n",
        "     Where:\n",
        "      * S = dataset at the current node\n",
        "      * A = feature being considered for the split\n",
        "      * Sv = subset of S where feature A has value v\n",
        "      * Entropy(S) = impurity before split\n",
        "      * Weighted average entropy of children = impurity after split\n",
        "  * Why is it Important?\n",
        "     * A Decision Tree can split on many possible features.\n",
        "     * Information Gain tells us how much a split improves purity:\n",
        "       * Higher IG = Better split (more reduction in uncertainty).\n",
        "       * The tree chooses the feature with the highest Information Gain at each step.\n",
        "  * Example:\n",
        "\n",
        "     Suppose we want to predict if students will \"Pass\" or \"Fail\" based on \"Hours Studied\":\n",
        "     * Parent Node: 50% Pass, 50% Fail →\n",
        "\n",
        "      Entropy=1.0\n",
        "     * After splitting on \"Hours Studied > 5\":\n",
        "       * Left child: 80% Pass, 20% Fail → Entropy = 0.72\n",
        "       * Right child: 20% Pass, 80% Fail → Entropy = 0.72\n",
        "     * Weighted avg child entropy = 0.72\n",
        "     * Information Gain = 1.0 − 0.72 = 0.28\n",
        "     "
      ],
      "metadata": {
        "id": "5ou29Bsf74Z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        " - **Real-World Applications of Decision Trees**\n",
        "   1. Finance & Banking\n",
        "      * Credit scoring → Predict if a customer is a “good” or “bad” loan risk.\n",
        "      * Fraud detection → Classify whether a transaction is suspicious.\n",
        "   2. Healthcare\n",
        "      * Disease diagnosis → Predict whether a patient has a particular condition (e.g., diabetes, cancer).\n",
        "      * Treatment recommendation → Decide on suitable treatment plans based on patient data.\n",
        "   3. Marketing & E-commerce\n",
        "      * Customer segmentation → Identify high-value customers.\n",
        "      * Churn prediction → Predict if a customer will stop using a service.\n",
        "      * Product recommendation → Suggest products based on user behavior.\n",
        "   4. Manufacturing & Operations\n",
        "      * Quality control → Detect defective vs. non-defective products.\n",
        "      * Predictive maintenance → Decide when a machine is likely to fail.\n",
        "   5. Telecommunications / IT\n",
        "      * Network intrusion detection → Identify abnormal usage patterns.\n",
        "      * Spam filtering → Classify emails as spam or not spam.\n",
        " * **Main Advantages**\n",
        "   1. Easy to interpret & visualize\n",
        "      * Trees look like flowcharts, so even non-technical stakeholders can understand.\n",
        "   2. Handles both numerical and categorical data\n",
        "      * Works well without much preprocessing (no need for scaling/normalization).\n",
        "   3. Captures non-linear relationships\n",
        "      * Can model complex decision boundaries.\n",
        "   4. Feature selection built-in\n",
        "      * Automatically chooses the most informative features during splitting.\n",
        " * **Main Limitations**\n",
        "   1. Overfitting\n",
        "      * Trees can grow too deep, memorizing noise in the training data.\n",
        "   2. High variance\n",
        "      * Small changes in data can produce very different trees.\n",
        "   3. Bias toward features with more levels\n",
        "      * Features with many unique values (like IDs, zip codes) can dominate splits.\n",
        "   4. Not always the best predictive performance\n",
        "      * Standalone Decision Trees may be weaker than ensemble methods like Random Forests or Gradient Boosted Trees.\n",
        "\n",
        "   Decision Trees are powerful because they’re simple, interpretable, and versatile — used in finance, healthcare, marketing, and beyond. But they need pruning or ensembling to overcome overfitting and instability."
      ],
      "metadata": {
        "id": "3ztskvZVLZY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "     \n",
        "     ● Load the Iris Dataset\n",
        "     \n",
        "     ● Train a Decision Tree Classifier using the Gini criterion\n",
        "     \n",
        "     ● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "_31lLRAmNk29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HtsddjZ2Gf6",
        "outputId": "b1a5021f-86e9-424c-f623-373af85d80c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.93\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0286\n",
            "petal length (cm): 0.5412\n",
            "petal width (cm): 0.4303\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into train and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this program does\n",
        "* Loads the Iris dataset from sklearn.datasets.\n",
        "* Splits into training (70%) and test (30%) sets.\n",
        "* Trains a Decision Tree Classifier with the Gini index.\n",
        "* Evaluates model accuracy on the test set.\n",
        "* Prints feature importances to show which features the tree found most useful."
      ],
      "metadata": {
        "id": "bxt45ahZN11m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "     \n",
        "     ● Load the Iris Dataset\n",
        "     \n",
        "     ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "      a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "InjlQU3gN92l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree with max_depth=3 (Pre-Pruned Tree)\n",
        "clf_shallow = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_shallow.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a fully grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred_shallow = clf_shallow.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# 6. Accuracy\n",
        "acc_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {acc_shallow:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {acc_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Qvz7m6Nt_m",
        "outputId": "24176bcf-9b6e-4061-ddd6-1d53277355a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 0.98\n",
            "Accuracy with fully grown tree: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does\n",
        "* Loads the Iris dataset.\n",
        "* Splits into train/test sets.\n",
        "* Trains:\n",
        "  * A pre-pruned tree (max_depth=3).\n",
        "  * A fully grown tree (no depth restriction).\n",
        "* Compares their accuracies on the test set."
      ],
      "metadata": {
        "id": "YqRbyFc6OQYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "    \n",
        "     ● Load the California Housing dataset from sklearn\n",
        "     \n",
        "     ● Train a Decision Tree Regressor\n",
        "     \n",
        "     ● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "JbXDlYKxObJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyf2pbl0OMct",
        "outputId": "658442b7-cc96-4091-cc18-e3b37907a749"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this program does\n",
        "* Loads the California Housing dataset.\n",
        "* Splits into training (80%) and test (20%) sets.\n",
        "* Trains a Decision Tree Regressor.\n",
        "* Evaluates the model using Mean Squared Error (MSE).\n",
        "* Prints feature importances to show which housing features influence price predictions most."
      ],
      "metadata": {
        "id": "A1HkMGhcOqBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "     \n",
        "     ● Load the Iris Dataset\n",
        "     \n",
        "     ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "      GridSearchCV\n",
        "     \n",
        "     ● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "8mBOcXYdOyVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 4. Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 5. Perform GridSearchCV (5-fold cross-validation)\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Evaluate best model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb46UnwoOmAH",
        "outputId": "7f18c31f-7d58-45fb-d307-6240b3c18fb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Test Set Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does\n",
        "* Loads the Iris dataset.\n",
        "* Splits into training (70%) and test (30%).\n",
        "* Defines a grid of hyperparameters (max_depth, min_samples_split).\n",
        "* Uses GridSearchCV with 5-fold cross-validation to find the best combination.\n",
        "* Prints the best parameters and evaluates the accuracy on the test set."
      ],
      "metadata": {
        "id": "nGiPK9MVPD2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. : Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "     ● Handle the missing values\n",
        "     \n",
        "     ● Encode the categorical features\n",
        "     \n",
        "     ● Train a Decision Tree model\n",
        "     \n",
        "     ● Tune its hyperparameters\n",
        "     \n",
        "     ● Evaluate its performance\n",
        "       And describe what business value this model could provide in the real-world setting.\n"
      ],
      "metadata": {
        "id": "oVKdM_QFPOgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Step-by-step process for building a disease-prediction Decision Tree (practical, healthcare focus)\n",
        "  1. Understand the data & split correctly\n",
        "  2. Handling missing values\n",
        "  3. Encoding categorical features\n",
        "  4. Feature engineering & selection (brief)\n",
        "  5. Training a Decision Tree model (practically)\n",
        "  6. Hyperparameter tuning\n",
        "  7. Evaluation (metrics & validation)\n",
        "  8. Deployment & monitoring (short)\n",
        "  9. Business value (real-world benefits & caveats)"
      ],
      "metadata": {
        "id": "8yUAvZH_Pxno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example pipeline: numeric impute (median) + categorical one-hot, DecisionTree, GridSearchCV\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# X, y = your features and binary label (1 = disease, 0 = no disease)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# identify column lists\n",
        "num_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    # trees don't need scaling, so we skip scaler\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "])\n",
        "\n",
        "preproc = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_cols),\n",
        "    (\"cat\", cat_pipeline, cat_cols)\n",
        "])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"preproc\", preproc),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
        "    \"clf__max_depth\": [3, 5, 7, None],\n",
        "    \"clf__min_samples_split\": [2, 5, 10],\n",
        "    \"clf__min_samples_leaf\": [1, 2, 5],\n",
        "    \"clf__class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "best = grid.best_estimator_\n",
        "\n",
        "y_pred = best.predict(X_test)\n",
        "y_proba = best.predict_proba(X_test)[:,1]\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "019slKbdQhzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}